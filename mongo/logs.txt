
==> Audit <==
|-----------|-------------------------|----------|------|---------|---------------------|---------------------|
|  Command  |          Args           | Profile  | User | Version |     Start Time      |      End Time       |
|-----------|-------------------------|----------|------|---------|---------------------|---------------------|
| start     |                         | minikube | root | v1.35.0 | 16 Apr 25 02:35 IST |                     |
| start     | --force                 | minikube | root | v1.35.0 | 16 Apr 25 02:36 IST | 16 Apr 25 02:39 IST |
| kubectl   | -- get pods -A          | minikube | root | v1.35.0 | 16 Apr 25 02:44 IST | 16 Apr 25 02:44 IST |
| start     | --force                 | minikube | root | v1.35.0 | 16 Apr 25 02:46 IST |                     |
| start     | --vm-driver=hyperkit    | minikube | root | v1.35.0 | 16 Apr 25 03:00 IST |                     |
| delete    |                         | minikube | root | v1.35.0 | 16 Apr 25 03:00 IST | 16 Apr 25 03:00 IST |
| dashboard |                         | minikube | root | v1.35.0 | 16 Apr 25 03:00 IST |                     |
| start     |                         | minikube | root | v1.35.0 | 16 Apr 25 03:01 IST |                     |
| start     | --force                 | minikube | root | v1.35.0 | 16 Apr 25 03:01 IST |                     |
| start     | --driver=none           | minikube | root | v1.35.0 | 16 Apr 25 03:01 IST |                     |
| start     |                         | minikube | root | v1.35.0 | 16 Apr 25 03:02 IST |                     |
| start     | --force                 | minikube | root | v1.35.0 | 16 Apr 25 03:02 IST |                     |
| start     | --force                 | minikube | root | v1.35.0 | 16 Apr 25 03:04 IST |                     |
| start     |                         | minikube | root | v1.35.0 | 16 Apr 25 03:07 IST |                     |
| start     | --force                 | minikube | root | v1.35.0 | 16 Apr 25 03:08 IST | 16 Apr 25 03:08 IST |
| start     | --driver=docker         | minikube | root | v1.35.0 | 16 Apr 25 03:11 IST |                     |
| start     | --driver=docker --force | minikube | root | v1.35.0 | 16 Apr 25 03:11 IST |                     |
| delete    |                         | minikube | root | v1.35.0 | 16 Apr 25 03:12 IST | 16 Apr 25 03:12 IST |
| dashboard |                         | minikube | root | v1.35.0 | 16 Apr 25 03:12 IST |                     |
| start     | --driver=docker --force | minikube | root | v1.35.0 | 16 Apr 25 03:12 IST |                     |
| start     | --driver=docker --force | minikube | root | v1.35.0 | 16 Apr 25 03:13 IST | 16 Apr 25 03:14 IST |
| dashboard |                         | minikube | root | v1.35.0 | 16 Apr 25 03:14 IST |                     |
| pause     |                         | minikube | root | v1.35.0 | 16 Apr 25 04:45 IST | 16 Apr 25 04:45 IST |
| dashboard |                         | minikube | root | v1.35.0 | 16 Apr 25 04:45 IST |                     |
| start     |                         | minikube | root | v1.35.0 | 18 Apr 25 19:45 IST |                     |
| start     | --force                 | minikube | root | v1.35.0 | 18 Apr 25 19:45 IST | 18 Apr 25 19:46 IST |
| dashboard |                         | minikube | root | v1.35.0 | 18 Apr 25 19:47 IST |                     |
| dashboard |                         | minikube | root | v1.35.0 | 18 Apr 25 21:18 IST |                     |
| pause     |                         | minikube | root | v1.35.0 | 18 Apr 25 22:24 IST | 18 Apr 25 22:24 IST |
| stop      |                         | minikube | root | v1.35.0 | 18 Apr 25 22:24 IST |                     |
| start     |                         | minikube | root | v1.35.0 | 19 Apr 25 18:23 IST |                     |
| start     | --force                 | minikube | root | v1.35.0 | 19 Apr 25 18:23 IST | 19 Apr 25 18:24 IST |
| dashboard |                         | minikube | root | v1.35.0 | 21 Apr 25 14:12 IST |                     |
| start     | --force                 | minikube | root | v1.35.0 | 21 Apr 25 14:12 IST | 21 Apr 25 14:12 IST |
| service   |                         | minikube | root | v1.35.0 | 21 Apr 25 15:13 IST |                     |
| service   | mongo-express-service   | minikube | root | v1.35.0 | 21 Apr 25 15:13 IST |                     |
| service   | mongo-express-service   | minikube | root | v1.35.0 | 21 Apr 25 15:17 IST |                     |
| service   | mongo-express-service   | minikube | root | v1.35.0 | 21 Apr 25 15:26 IST |                     |
| service   | mongo-express-service   | minikube | root | v1.35.0 | 21 Apr 25 15:26 IST |                     |
| pause     |                         | minikube | root | v1.35.0 | 21 Apr 25 15:29 IST | 21 Apr 25 15:29 IST |
| dashboard |                         | minikube | root | v1.35.0 | 21 Apr 25 15:29 IST |                     |
| start     |                         | minikube | root | v1.35.0 | 22 Apr 25 09:57 IST |                     |
| start     | --force                 | minikube | root | v1.35.0 | 22 Apr 25 09:58 IST | 22 Apr 25 09:58 IST |
| service   | mongo-express-service   | minikube | root | v1.35.0 | 22 Apr 25 10:01 IST |                     |
| service   | mongo-express-service   | minikube | root | v1.35.0 | 22 Apr 25 10:10 IST |                     |
| service   | mongo-express-service   | minikube | root | v1.35.0 | 22 Apr 25 10:10 IST |                     |
|-----------|-------------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/04/22 09:58:00
Running on machine: rd-Mi-NoteBook-Pro
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0422 09:58:00.074322  230336 out.go:345] Setting OutFile to fd 1 ...
I0422 09:58:00.074479  230336 out.go:397] isatty.IsTerminal(1) = true
I0422 09:58:00.074486  230336 out.go:358] Setting ErrFile to fd 2...
I0422 09:58:00.074495  230336 out.go:397] isatty.IsTerminal(2) = true
I0422 09:58:00.074858  230336 root.go:338] Updating PATH: /root/.minikube/bin
I0422 09:58:00.075535  230336 out.go:352] Setting JSON to false
I0422 09:58:00.078109  230336 start.go:129] hostinfo: {"hostname":"rd-Mi-NoteBook-Pro","uptime":7243,"bootTime":1745288837,"procs":314,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.8.0-57-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"20211016-0091-9e46-aa3c-00919e46aa40"}
I0422 09:58:00.078207  230336 start.go:139] virtualization: kvm host
I0422 09:58:00.079804  230336 out.go:177] 😄  minikube v1.35.0 on Ubuntu 24.04
W0422 09:58:00.081210  230336 out.go:270] ❗  minikube skips various validations when --force is supplied; this may lead to unexpected behavior
I0422 09:58:00.081300  230336 notify.go:220] Checking for updates...
I0422 09:58:00.082482  230336 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0422 09:58:00.082708  230336 driver.go:394] Setting default libvirt URI to qemu:///system
I0422 09:58:00.118736  230336 docker.go:123] docker version: linux-28.1.1:Docker Engine - Community
I0422 09:58:00.118786  230336 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0422 09:58:00.184061  230336 info.go:266] docker info: {ID:d7b5d310-d398-4f99-8703-124403e63a12 Containers:6 ContainersRunning:0 ContainersPaused:0 ContainersStopped:6 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:44 SystemTime:2025-04-22 09:58:00.176506887 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-57-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16542224384 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:rd-Mi-NoteBook-Pro Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1-desktop.1] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7-desktop.1] map[Name:debug Path:/usr/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:/usr/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:/usr/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/usr/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.14.0]] Warnings:<nil>}}
I0422 09:58:00.184168  230336 docker.go:318] overlay module found
I0422 09:58:00.185146  230336 out.go:177] ✨  Using the docker driver based on existing profile
I0422 09:58:00.185908  230336 start.go:297] selected driver: docker
I0422 09:58:00.185917  230336 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0422 09:58:00.186005  230336 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
W0422 09:58:00.186074  230336 out.go:270] 🛑  The "docker" driver should not be used with root privileges. If you wish to continue as root, use --force.
W0422 09:58:00.186089  230336 out.go:270] 💡  If you are running minikube within a VM, consider using --driver=none:
W0422 09:58:00.186139  230336 out.go:270] 📘    https://minikube.sigs.k8s.io/docs/reference/drivers/none/
I0422 09:58:00.186317  230336 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
W0422 09:58:00.186335  230336 out.go:270] 💡  Tip: To remove this root owned cluster, run: sudo minikube delete
I0422 09:58:00.186388  230336 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0422 09:58:00.260454  230336 info.go:266] docker info: {ID:d7b5d310-d398-4f99-8703-124403e63a12 Containers:6 ContainersRunning:0 ContainersPaused:0 ContainersStopped:6 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:44 SystemTime:2025-04-22 09:58:00.251984947 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-57-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16542224384 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:rd-Mi-NoteBook-Pro Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1-desktop.1] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7-desktop.1] map[Name:debug Path:/usr/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:/usr/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:/usr/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/usr/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.14.0]] Warnings:<nil>}}
I0422 09:58:00.261199  230336 cni.go:84] Creating CNI manager for ""
I0422 09:58:00.261245  230336 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0422 09:58:00.261292  230336 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0422 09:58:00.262312  230336 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0422 09:58:00.263042  230336 cache.go:121] Beginning downloading kic base image for docker with docker
I0422 09:58:00.263981  230336 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0422 09:58:00.264793  230336 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0422 09:58:00.264841  230336 preload.go:146] Found local preload: /root/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0422 09:58:00.264846  230336 cache.go:56] Caching tarball of preloaded images
I0422 09:58:00.264846  230336 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0422 09:58:00.264983  230336 preload.go:172] Found /root/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0422 09:58:00.264989  230336 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0422 09:58:00.265071  230336 profile.go:143] Saving config to /root/.minikube/profiles/minikube/config.json ...
I0422 09:58:00.285052  230336 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0422 09:58:00.285062  230336 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0422 09:58:00.285071  230336 cache.go:227] Successfully downloaded all kic artifacts
I0422 09:58:00.285088  230336 start.go:360] acquireMachinesLock for minikube: {Name:mke11f63b5835bf422927bf558fccac7a21a838f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0422 09:58:00.285165  230336 start.go:364] duration metric: took 65.317µs to acquireMachinesLock for "minikube"
I0422 09:58:00.285174  230336 start.go:96] Skipping create...Using existing machine configuration
I0422 09:58:00.285178  230336 fix.go:54] fixHost starting: 
I0422 09:58:00.285351  230336 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0422 09:58:00.300363  230336 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0422 09:58:00.300382  230336 fix.go:138] unexpected machine state, will restart: <nil>
I0422 09:58:00.301434  230336 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0422 09:58:00.302837  230336 cli_runner.go:164] Run: docker start minikube
I0422 09:58:00.665187  230336 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0422 09:58:00.679332  230336 kic.go:430] container "minikube" state is running.
I0422 09:58:00.679610  230336 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0422 09:58:00.694240  230336 profile.go:143] Saving config to /root/.minikube/profiles/minikube/config.json ...
I0422 09:58:00.694420  230336 machine.go:93] provisionDockerMachine start ...
I0422 09:58:00.694462  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:00.708543  230336 main.go:141] libmachine: Using SSH client type: native
I0422 09:58:00.708874  230336 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0422 09:58:00.708880  230336 main.go:141] libmachine: About to run SSH command:
hostname
I0422 09:58:00.709449  230336 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:34988->127.0.0.1:32768: read: connection reset by peer
I0422 09:58:03.878862  230336 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0422 09:58:03.878882  230336 ubuntu.go:169] provisioning hostname "minikube"
I0422 09:58:03.878974  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:03.908505  230336 main.go:141] libmachine: Using SSH client type: native
I0422 09:58:03.908807  230336 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0422 09:58:03.908831  230336 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0422 09:58:04.106782  230336 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0422 09:58:04.106872  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:04.135472  230336 main.go:141] libmachine: Using SSH client type: native
I0422 09:58:04.135749  230336 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0422 09:58:04.135771  230336 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0422 09:58:04.298070  230336 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0422 09:58:04.298111  230336 ubuntu.go:175] set auth options {CertDir:/root/.minikube CaCertPath:/root/.minikube/certs/ca.pem CaPrivateKeyPath:/root/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/root/.minikube/machines/server.pem ServerKeyPath:/root/.minikube/machines/server-key.pem ClientKeyPath:/root/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/root/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/root/.minikube}
I0422 09:58:04.298150  230336 ubuntu.go:177] setting up certificates
I0422 09:58:04.298160  230336 provision.go:84] configureAuth start
I0422 09:58:04.298248  230336 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0422 09:58:04.326833  230336 provision.go:143] copyHostCerts
I0422 09:58:04.329009  230336 exec_runner.go:144] found /root/.minikube/ca.pem, removing ...
I0422 09:58:04.329286  230336 exec_runner.go:203] rm: /root/.minikube/ca.pem
I0422 09:58:04.329372  230336 exec_runner.go:151] cp: /root/.minikube/certs/ca.pem --> /root/.minikube/ca.pem (1070 bytes)
I0422 09:58:04.329672  230336 exec_runner.go:144] found /root/.minikube/cert.pem, removing ...
I0422 09:58:04.329684  230336 exec_runner.go:203] rm: /root/.minikube/cert.pem
I0422 09:58:04.329729  230336 exec_runner.go:151] cp: /root/.minikube/certs/cert.pem --> /root/.minikube/cert.pem (1115 bytes)
I0422 09:58:04.329974  230336 exec_runner.go:144] found /root/.minikube/key.pem, removing ...
I0422 09:58:04.329981  230336 exec_runner.go:203] rm: /root/.minikube/key.pem
I0422 09:58:04.330026  230336 exec_runner.go:151] cp: /root/.minikube/certs/key.pem --> /root/.minikube/key.pem (1679 bytes)
I0422 09:58:04.330254  230336 provision.go:117] generating server cert: /root/.minikube/machines/server.pem ca-key=/root/.minikube/certs/ca.pem private-key=/root/.minikube/certs/ca-key.pem org=root.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0422 09:58:04.570250  230336 provision.go:177] copyRemoteCerts
I0422 09:58:04.572084  230336 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0422 09:58:04.572132  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:04.597827  230336 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0422 09:58:04.716815  230336 ssh_runner.go:362] scp /root/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0422 09:58:04.767566  230336 ssh_runner.go:362] scp /root/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0422 09:58:04.813052  230336 ssh_runner.go:362] scp /root/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0422 09:58:04.856230  230336 provision.go:87] duration metric: took 558.054654ms to configureAuth
I0422 09:58:04.856253  230336 ubuntu.go:193] setting minikube options for container-runtime
I0422 09:58:04.856547  230336 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0422 09:58:04.856602  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:04.884266  230336 main.go:141] libmachine: Using SSH client type: native
I0422 09:58:04.884546  230336 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0422 09:58:04.884559  230336 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0422 09:58:05.045215  230336 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0422 09:58:05.045231  230336 ubuntu.go:71] root file system type: overlay
I0422 09:58:05.045406  230336 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0422 09:58:05.045491  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:05.064208  230336 main.go:141] libmachine: Using SSH client type: native
I0422 09:58:05.064392  230336 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0422 09:58:05.064466  230336 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0422 09:58:05.246252  230336 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0422 09:58:05.246366  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:05.273903  230336 main.go:141] libmachine: Using SSH client type: native
I0422 09:58:05.274186  230336 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0422 09:58:05.274211  230336 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0422 09:58:05.446827  230336 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0422 09:58:05.446847  230336 machine.go:96] duration metric: took 4.752418485s to provisionDockerMachine
I0422 09:58:05.446859  230336 start.go:293] postStartSetup for "minikube" (driver="docker")
I0422 09:58:05.446874  230336 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0422 09:58:05.446946  230336 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0422 09:58:05.446988  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:05.474761  230336 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0422 09:58:05.593570  230336 ssh_runner.go:195] Run: cat /etc/os-release
I0422 09:58:05.599062  230336 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0422 09:58:05.599100  230336 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0422 09:58:05.599113  230336 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0422 09:58:05.599121  230336 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0422 09:58:05.599134  230336 filesync.go:126] Scanning /root/.minikube/addons for local assets ...
I0422 09:58:05.599372  230336 filesync.go:126] Scanning /root/.minikube/files for local assets ...
I0422 09:58:05.599531  230336 start.go:296] duration metric: took 152.664909ms for postStartSetup
I0422 09:58:05.599548  230336 fix.go:56] duration metric: took 5.314369446s for fixHost
I0422 09:58:05.599554  230336 start.go:83] releasing machines lock for "minikube", held for 5.314383417s
I0422 09:58:05.599613  230336 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0422 09:58:05.627350  230336 ssh_runner.go:195] Run: cat /version.json
I0422 09:58:05.627392  230336 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0422 09:58:05.627407  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:05.627478  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:05.646374  230336 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0422 09:58:05.646660  230336 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0422 09:58:06.216171  230336 ssh_runner.go:195] Run: systemctl --version
I0422 09:58:06.225945  230336 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0422 09:58:06.233735  230336 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0422 09:58:06.268295  230336 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0422 09:58:06.268402  230336 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0422 09:58:06.284254  230336 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0422 09:58:06.284290  230336 start.go:495] detecting cgroup driver to use...
I0422 09:58:06.284328  230336 detect.go:190] detected "systemd" cgroup driver on host os
I0422 09:58:06.284516  230336 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0422 09:58:06.313504  230336 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0422 09:58:06.330870  230336 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0422 09:58:06.348612  230336 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0422 09:58:06.348670  230336 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0422 09:58:06.367184  230336 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0422 09:58:06.385201  230336 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0422 09:58:06.402296  230336 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0422 09:58:06.420104  230336 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0422 09:58:06.436653  230336 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0422 09:58:06.454283  230336 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0422 09:58:06.472018  230336 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0422 09:58:06.490539  230336 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0422 09:58:06.505797  230336 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0422 09:58:06.505857  230336 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I0422 09:58:06.526777  230336 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0422 09:58:06.542172  230336 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0422 09:58:06.643312  230336 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0422 09:58:06.775489  230336 start.go:495] detecting cgroup driver to use...
I0422 09:58:06.775544  230336 detect.go:190] detected "systemd" cgroup driver on host os
I0422 09:58:06.775610  230336 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0422 09:58:06.796397  230336 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0422 09:58:06.796480  230336 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0422 09:58:06.817782  230336 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0422 09:58:06.850196  230336 ssh_runner.go:195] Run: which cri-dockerd
I0422 09:58:06.856328  230336 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0422 09:58:06.871950  230336 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0422 09:58:06.894727  230336 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0422 09:58:07.017395  230336 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0422 09:58:07.078978  230336 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0422 09:58:07.079055  230336 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0422 09:58:07.095412  230336 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0422 09:58:07.152199  230336 ssh_runner.go:195] Run: sudo systemctl restart docker
I0422 09:58:08.591989  230336 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.439740086s)
I0422 09:58:08.592064  230336 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0422 09:58:08.611372  230336 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0422 09:58:08.632285  230336 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0422 09:58:08.652879  230336 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0422 09:58:08.747922  230336 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0422 09:58:08.811066  230336 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0422 09:58:08.888979  230336 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0422 09:58:08.924060  230336 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0422 09:58:08.943017  230336 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0422 09:58:09.043860  230336 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0422 09:58:09.228755  230336 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0422 09:58:09.228806  230336 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0422 09:58:09.232002  230336 start.go:563] Will wait 60s for crictl version
I0422 09:58:09.232041  230336 ssh_runner.go:195] Run: which crictl
I0422 09:58:09.234678  230336 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0422 09:58:09.317542  230336 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0422 09:58:09.317597  230336 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0422 09:58:09.376395  230336 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0422 09:58:09.396215  230336 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0422 09:58:09.396287  230336 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0422 09:58:09.413066  230336 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0422 09:58:09.416181  230336 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0422 09:58:09.426494  230336 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0422 09:58:09.426573  230336 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0422 09:58:09.426614  230336 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0422 09:58:09.458412  230336 docker.go:689] Got preloaded images: -- stdout --
mongo:latest
nginx:1.27.4
nginx:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
mongo-express:latest
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.16

-- /stdout --
I0422 09:58:09.458427  230336 docker.go:619] Images already preloaded, skipping extraction
I0422 09:58:09.458499  230336 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0422 09:58:09.492780  230336 docker.go:689] Got preloaded images: -- stdout --
mongo:latest
nginx:1.27.4
nginx:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
mongo-express:latest
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.16

-- /stdout --
I0422 09:58:09.492798  230336 cache_images.go:84] Images are preloaded, skipping loading
I0422 09:58:09.492809  230336 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0422 09:58:09.493017  230336 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0422 09:58:09.493086  230336 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0422 09:58:09.649917  230336 cni.go:84] Creating CNI manager for ""
I0422 09:58:09.649943  230336 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0422 09:58:09.649980  230336 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0422 09:58:09.649997  230336 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0422 09:58:09.650095  230336 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0422 09:58:09.650149  230336 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0422 09:58:09.658550  230336 binaries.go:44] Found k8s binaries, skipping transfer
I0422 09:58:09.658608  230336 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0422 09:58:09.665579  230336 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0422 09:58:09.695700  230336 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0422 09:58:09.729739  230336 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0422 09:58:09.762826  230336 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0422 09:58:09.768487  230336 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0422 09:58:09.787879  230336 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0422 09:58:09.892267  230336 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0422 09:58:09.917081  230336 certs.go:68] Setting up /root/.minikube/profiles/minikube for IP: 192.168.49.2
I0422 09:58:09.917094  230336 certs.go:194] generating shared ca certs ...
I0422 09:58:09.917114  230336 certs.go:226] acquiring lock for ca certs: {Name:mkb814c315fe9b7fabb439d6d58c5448fbb7853c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0422 09:58:09.917524  230336 certs.go:235] skipping valid "minikubeCA" ca cert: /root/.minikube/ca.key
I0422 09:58:09.917705  230336 certs.go:235] skipping valid "proxyClientCA" ca cert: /root/.minikube/proxy-client-ca.key
I0422 09:58:09.917715  230336 certs.go:256] generating profile certs ...
I0422 09:58:09.917952  230336 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /root/.minikube/profiles/minikube/client.key
I0422 09:58:09.918154  230336 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /root/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0422 09:58:09.918327  230336 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /root/.minikube/profiles/minikube/proxy-client.key
I0422 09:58:09.918494  230336 certs.go:484] found cert: /root/.minikube/certs/ca-key.pem (1679 bytes)
I0422 09:58:09.918540  230336 certs.go:484] found cert: /root/.minikube/certs/ca.pem (1070 bytes)
I0422 09:58:09.918570  230336 certs.go:484] found cert: /root/.minikube/certs/cert.pem (1115 bytes)
I0422 09:58:09.918597  230336 certs.go:484] found cert: /root/.minikube/certs/key.pem (1679 bytes)
I0422 09:58:09.919507  230336 ssh_runner.go:362] scp /root/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0422 09:58:09.964928  230336 ssh_runner.go:362] scp /root/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0422 09:58:10.013395  230336 ssh_runner.go:362] scp /root/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0422 09:58:10.058520  230336 ssh_runner.go:362] scp /root/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0422 09:58:10.083701  230336 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0422 09:58:10.118456  230336 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0422 09:58:10.154187  230336 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0422 09:58:10.211595  230336 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0422 09:58:10.247948  230336 ssh_runner.go:362] scp /root/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0422 09:58:10.274338  230336 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0422 09:58:10.296514  230336 ssh_runner.go:195] Run: openssl version
I0422 09:58:10.305089  230336 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0422 09:58:10.320981  230336 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0422 09:58:10.325837  230336 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Apr 15 21:09 /usr/share/ca-certificates/minikubeCA.pem
I0422 09:58:10.325882  230336 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0422 09:58:10.331933  230336 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0422 09:58:10.341339  230336 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0422 09:58:10.344626  230336 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0422 09:58:10.350361  230336 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0422 09:58:10.356980  230336 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0422 09:58:10.363281  230336 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0422 09:58:10.368814  230336 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0422 09:58:10.375631  230336 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0422 09:58:10.381652  230336 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0422 09:58:10.381763  230336 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0422 09:58:10.398031  230336 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0422 09:58:10.408396  230336 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0422 09:58:10.408429  230336 kubeadm.go:593] restartPrimaryControlPlane start ...
I0422 09:58:10.408474  230336 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0422 09:58:10.417516  230336 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0422 09:58:10.418368  230336 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I0422 09:58:10.426675  230336 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0422 09:58:10.434652  230336 kubeadm.go:630] The running cluster does not require reconfiguration: 192.168.49.2
I0422 09:58:10.434671  230336 kubeadm.go:597] duration metric: took 26.237188ms to restartPrimaryControlPlane
I0422 09:58:10.434679  230336 kubeadm.go:394] duration metric: took 53.035442ms to StartCluster
I0422 09:58:10.434690  230336 settings.go:142] acquiring lock: {Name:mk19004591210340446308469f521c5cfa3e1599 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0422 09:58:10.434789  230336 settings.go:150] Updating kubeconfig:  /root/.kube/config
I0422 09:58:10.435284  230336 lock.go:35] WriteFile acquiring /root/.kube/config: {Name:mk72a1487fd2da23da9e8181e16f352a6105bd56 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0422 09:58:10.435497  230336 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0422 09:58:10.435539  230336 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0422 09:58:10.435623  230336 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0422 09:58:10.435639  230336 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0422 09:58:10.435645  230336 addons.go:247] addon storage-provisioner should already be in state true
I0422 09:58:10.435645  230336 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0422 09:58:10.435647  230336 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0422 09:58:10.435643  230336 addons.go:69] Setting dashboard=true in profile "minikube"
I0422 09:58:10.435662  230336 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0422 09:58:10.435670  230336 addons.go:238] Setting addon dashboard=true in "minikube"
I0422 09:58:10.435670  230336 host.go:66] Checking if "minikube" exists ...
W0422 09:58:10.435684  230336 addons.go:247] addon dashboard should already be in state true
I0422 09:58:10.435721  230336 host.go:66] Checking if "minikube" exists ...
I0422 09:58:10.435945  230336 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0422 09:58:10.436022  230336 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0422 09:58:10.436128  230336 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0422 09:58:10.436878  230336 out.go:177] 🔎  Verifying Kubernetes components...
I0422 09:58:10.439209  230336 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0422 09:58:10.463882  230336 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0422 09:58:10.463899  230336 addons.go:247] addon default-storageclass should already be in state true
I0422 09:58:10.463940  230336 host.go:66] Checking if "minikube" exists ...
I0422 09:58:10.464552  230336 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0422 09:58:10.465201  230336 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0422 09:58:10.466581  230336 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0422 09:58:10.467702  230336 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0422 09:58:10.467723  230336 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0422 09:58:10.467823  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:10.475304  230336 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0422 09:58:10.476740  230336 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0422 09:58:10.476760  230336 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0422 09:58:10.476845  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:10.510465  230336 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0422 09:58:10.510489  230336 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0422 09:58:10.510481  230336 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0422 09:58:10.510540  230336 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0422 09:58:10.515468  230336 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0422 09:58:10.532323  230336 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0422 09:58:10.560599  230336 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0422 09:58:10.574389  230336 api_server.go:52] waiting for apiserver process to appear ...
I0422 09:58:10.574440  230336 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0422 09:58:10.629880  230336 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0422 09:58:10.629882  230336 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0422 09:58:10.629894  230336 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0422 09:58:10.643684  230336 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0422 09:58:10.667796  230336 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0422 09:58:10.667815  230336 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0422 09:58:10.691621  230336 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0422 09:58:10.691638  230336 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0422 09:58:10.711355  230336 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0422 09:58:10.711369  230336 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0422 09:58:10.735531  230336 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0422 09:58:10.735551  230336 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0422 09:58:10.774037  230336 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0422 09:58:10.774057  230336 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0422 09:58:10.812890  230336 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0422 09:58:10.812910  230336 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0422 09:58:10.845231  230336 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0422 09:58:10.845246  230336 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
W0422 09:58:10.857407  230336 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0422 09:58:10.857433  230336 retry.go:31] will retry after 211.120935ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0422 09:58:10.857740  230336 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0422 09:58:10.857753  230336 retry.go:31] will retry after 357.356814ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0422 09:58:10.869426  230336 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0422 09:58:10.869442  230336 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0422 09:58:10.891614  230336 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0422 09:58:10.950538  230336 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0422 09:58:10.950593  230336 retry.go:31] will retry after 296.542031ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0422 09:58:11.069470  230336 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0422 09:58:11.074985  230336 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0422 09:58:11.146384  230336 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0422 09:58:11.146402  230336 retry.go:31] will retry after 357.640828ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0422 09:58:11.215660  230336 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0422 09:58:11.247976  230336 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0422 09:58:11.305534  230336 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0422 09:58:11.305568  230336 retry.go:31] will retry after 439.912391ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0422 09:58:11.375630  230336 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0422 09:58:11.375657  230336 retry.go:31] will retry after 379.649705ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0422 09:58:11.504454  230336 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0422 09:58:11.574571  230336 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0422 09:58:11.746324  230336 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0422 09:58:11.756288  230336 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0422 09:58:13.512831  230336 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.008342209s)
I0422 09:58:13.512842  230336 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.938253688s)
I0422 09:58:13.512874  230336 api_server.go:72] duration metric: took 3.077357432s to wait for apiserver process to appear ...
I0422 09:58:13.512879  230336 api_server.go:88] waiting for apiserver healthz status ...
I0422 09:58:13.512898  230336 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0422 09:58:13.512896  230336 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.766557175s)
I0422 09:58:13.523075  230336 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0422 09:58:13.523101  230336 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0422 09:58:14.013513  230336 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0422 09:58:14.018910  230336 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0422 09:58:14.018927  230336 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0422 09:58:14.513041  230336 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0422 09:58:14.521024  230336 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0422 09:58:14.522573  230336 api_server.go:141] control plane version: v1.32.0
I0422 09:58:14.522594  230336 api_server.go:131] duration metric: took 1.009709054s to wait for apiserver health ...
I0422 09:58:14.522601  230336 system_pods.go:43] waiting for kube-system pods to appear ...
I0422 09:58:14.532779  230336 system_pods.go:59] 8 kube-system pods found
I0422 09:58:14.532801  230336 system_pods.go:61] "coredns-668d6bf9bc-2gfg2" [c65ea2c6-c4ea-4051-bfa4-6b0f8fcdc5dd] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0422 09:58:14.532811  230336 system_pods.go:61] "coredns-668d6bf9bc-5ktnp" [979a922e-7c23-4346-a791-5a47839dd5f0] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0422 09:58:14.532822  230336 system_pods.go:61] "etcd-minikube" [8a952d58-92c4-410f-a440-6097ee492956] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0422 09:58:14.532828  230336 system_pods.go:61] "kube-apiserver-minikube" [bd2b0ad1-ee4a-49d9-b74b-876d9dce9d53] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0422 09:58:14.532832  230336 system_pods.go:61] "kube-controller-manager-minikube" [1c50d4d3-6b2d-4f1c-b18f-30a7cfac78da] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0422 09:58:14.532836  230336 system_pods.go:61] "kube-proxy-pxh7n" [8ef83f2f-dd57-416f-9156-2ecf78429a8d] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0422 09:58:14.532841  230336 system_pods.go:61] "kube-scheduler-minikube" [2478dcb1-1ac1-4c9c-9c37-92b106e4f0a1] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0422 09:58:14.532852  230336 system_pods.go:61] "storage-provisioner" [c2c6b02f-ac4f-46b2-88e7-ce27af88e40c] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0422 09:58:14.532859  230336 system_pods.go:74] duration metric: took 10.251726ms to wait for pod list to return data ...
I0422 09:58:14.532869  230336 kubeadm.go:582] duration metric: took 4.097351425s to wait for: map[apiserver:true system_pods:true]
I0422 09:58:14.532881  230336 node_conditions.go:102] verifying NodePressure condition ...
I0422 09:58:14.536906  230336 node_conditions.go:122] node storage ephemeral capacity is 490617784Ki
I0422 09:58:14.536936  230336 node_conditions.go:123] node cpu capacity is 8
I0422 09:58:14.536949  230336 node_conditions.go:105] duration metric: took 4.063409ms to run NodePressure ...
I0422 09:58:14.536962  230336 start.go:241] waiting for startup goroutines ...
I0422 09:58:14.537544  230336 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (2.781202498s)
I0422 09:58:14.538818  230336 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0422 09:58:14.541600  230336 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass, dashboard
I0422 09:58:14.542854  230336 addons.go:514] duration metric: took 4.107305559s for enable addons: enabled=[storage-provisioner default-storageclass dashboard]
I0422 09:58:14.542907  230336 start.go:246] waiting for cluster config update ...
I0422 09:58:14.542926  230336 start.go:255] writing updated cluster config ...
I0422 09:58:14.543375  230336 ssh_runner.go:195] Run: rm -f paused
I0422 09:58:14.682612  230336 start.go:600] kubectl: 1.32.3, cluster: 1.32.0 (minor skew: 0)
I0422 09:58:14.683747  230336 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Apr 22 04:28:08 minikube dockerd[983]: time="2025-04-22T04:28:08.546188010Z" level=warning msg="error locating sandbox id 91fda4fd24c3801691379387c255428e441cb87918f1d9a54fe3410f25b916c9: sandbox 91fda4fd24c3801691379387c255428e441cb87918f1d9a54fe3410f25b916c9 not found"
Apr 22 04:28:08 minikube dockerd[983]: time="2025-04-22T04:28:08.546220366Z" level=warning msg="error locating sandbox id 2ace9660288b083b5eb0a3b12510e22970fcd9025dd28e2261381492e5812a54: sandbox 2ace9660288b083b5eb0a3b12510e22970fcd9025dd28e2261381492e5812a54 not found"
Apr 22 04:28:08 minikube dockerd[983]: time="2025-04-22T04:28:08.546266565Z" level=warning msg="error locating sandbox id b9d78d64899fb96cc65f5a3280765e810d1518c0f883e1ec568a8ef13fdf2023: sandbox b9d78d64899fb96cc65f5a3280765e810d1518c0f883e1ec568a8ef13fdf2023 not found"
Apr 22 04:28:08 minikube dockerd[983]: time="2025-04-22T04:28:08.546291210Z" level=warning msg="error locating sandbox id 748f60a3cac63a0a119b9c80134206bcdbc3ca4d5ec105f4af7aca13a7a456bc: sandbox 748f60a3cac63a0a119b9c80134206bcdbc3ca4d5ec105f4af7aca13a7a456bc not found"
Apr 22 04:28:08 minikube dockerd[983]: time="2025-04-22T04:28:08.546359329Z" level=warning msg="error locating sandbox id 06e685f32511de15371c5c2a2cecfff0ffd6462a8c7adaa566618393ed792c06: sandbox 06e685f32511de15371c5c2a2cecfff0ffd6462a8c7adaa566618393ed792c06 not found"
Apr 22 04:28:08 minikube dockerd[983]: time="2025-04-22T04:28:08.546388043Z" level=warning msg="error locating sandbox id d911aed119a1690b867085cdd0765f1b70fe51947a228e25917048d6da25f057: sandbox d911aed119a1690b867085cdd0765f1b70fe51947a228e25917048d6da25f057 not found"
Apr 22 04:28:08 minikube dockerd[983]: time="2025-04-22T04:28:08.546787889Z" level=info msg="Loading containers: done."
Apr 22 04:28:08 minikube dockerd[983]: time="2025-04-22T04:28:08.560367936Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Apr 22 04:28:08 minikube dockerd[983]: time="2025-04-22T04:28:08.560463695Z" level=info msg="Daemon has completed initialization"
Apr 22 04:28:08 minikube dockerd[983]: time="2025-04-22T04:28:08.588865836Z" level=info msg="API listen on /var/run/docker.sock"
Apr 22 04:28:08 minikube dockerd[983]: time="2025-04-22T04:28:08.588865845Z" level=info msg="API listen on [::]:2376"
Apr 22 04:28:08 minikube systemd[1]: Started Docker Application Container Engine.
Apr 22 04:28:09 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Apr 22 04:28:09 minikube cri-dockerd[1273]: time="2025-04-22T04:28:09Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Apr 22 04:28:09 minikube cri-dockerd[1273]: time="2025-04-22T04:28:09Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Apr 22 04:28:09 minikube cri-dockerd[1273]: time="2025-04-22T04:28:09Z" level=info msg="Start docker client with request timeout 0s"
Apr 22 04:28:09 minikube cri-dockerd[1273]: time="2025-04-22T04:28:09Z" level=info msg="Hairpin mode is set to hairpin-veth"
Apr 22 04:28:09 minikube cri-dockerd[1273]: time="2025-04-22T04:28:09Z" level=info msg="Loaded network plugin cni"
Apr 22 04:28:09 minikube cri-dockerd[1273]: time="2025-04-22T04:28:09Z" level=info msg="Docker cri networking managed by network plugin cni"
Apr 22 04:28:09 minikube cri-dockerd[1273]: time="2025-04-22T04:28:09Z" level=info msg="Setting cgroupDriver systemd"
Apr 22 04:28:09 minikube cri-dockerd[1273]: time="2025-04-22T04:28:09Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Apr 22 04:28:09 minikube cri-dockerd[1273]: time="2025-04-22T04:28:09Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Apr 22 04:28:09 minikube cri-dockerd[1273]: time="2025-04-22T04:28:09Z" level=info msg="Start cri-dockerd grpc backend"
Apr 22 04:28:09 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-express-5dd87b9fcf-qbtm7_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"26779fc201e61b98730fa0e6f7957c057cb83fa24663de790628d73181e3d4d4\""
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-7779f9b69b-8bpks_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ae3f112d9cc62346b8096213938398ca7f5e1d7965ce688e5c518f726065ac37\""
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongodb-deployment-6d9d7c68f6-86545_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"eb274c4ff7160ab891456a1848862fe8c8286a0fc68a50df9724149140c83a36\""
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongodb-deployment-6d9d7c68f6-86545_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"021c7ffa4c391b4200ad7a2beaa0029e2566f0e0976e5a353e9400166ee16c46\""
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-2gfg2_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8c1f111235a02552f61bdd5a973ea64e03badc4420dd4aee893d7100a0ba580a\""
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-2gfg2_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d1763b45d6416f07af700a890035f42f7590a5a18eab511ecb08151ba58e0bbe\""
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-5ktnp_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2afa4d7fefd124be6e3cbc82994c88d13e91168d4986a57307f827a8d5b3fba0\""
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-5ktnp_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c953da96ba5c8b0b742c53aabffc400f98fe8b13b7b01817a09cef86bef8cdf7\""
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-dkdsw_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8f6cca311b04d1a7acec01c037b04ce31fd018ebf1260ecdcf180ba9f961e35e\""
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-dkdsw_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f5693b500d11245c65e061626eedaeb9ce8236a97b1803713c3c3dd05031327d\""
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"3a0a099a0330fabcc383e8068924e91b5c18bbbe1961a1686296bc170f3af2ad\". Proceed without further sandbox information."
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"bfefb21f43cc8034165d6fdd250fee72ed2e243da3ceb764e2997f6fdf582116\". Proceed without further sandbox information."
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"c095242dad0387033fd49c688fa31494c5d535ca560756dc71a3ab49b3acf7bf\". Proceed without further sandbox information."
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"9d99b32ab2ea5ef88254cf71ad21b27799d1318214db8859e290d104e0b53ad4\". Proceed without further sandbox information."
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1acbb0ad8c175c90f9114f4d975f3062fe5ff8bf4eb6bed5355f824e02403e97/resolv.conf as [nameserver 192.168.49.1 search NSEROOT.COM options edns0 trust-ad ndots:0]"
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d1dd2e025fb733763b0bf0eef9dfcb110606cda82d56e75c07cae1da4c054745/resolv.conf as [nameserver 192.168.49.1 search NSEROOT.COM options edns0 trust-ad ndots:0]"
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/653764636e369c996033f840be2de6d6d146ff4ab0be640eff9dafff5636be4e/resolv.conf as [nameserver 192.168.49.1 search NSEROOT.COM options ndots:0 edns0 trust-ad]"
Apr 22 04:28:10 minikube cri-dockerd[1273]: time="2025-04-22T04:28:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4e335292c6d0c3aed271221e884e278c07cb09a44cdc683ab7e9220637a53fb3/resolv.conf as [nameserver 192.168.49.1 search NSEROOT.COM options edns0 trust-ad ndots:0]"
Apr 22 04:28:11 minikube cri-dockerd[1273]: time="2025-04-22T04:28:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-7779f9b69b-8bpks_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ae3f112d9cc62346b8096213938398ca7f5e1d7965ce688e5c518f726065ac37\""
Apr 22 04:28:11 minikube cri-dockerd[1273]: time="2025-04-22T04:28:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongodb-deployment-6d9d7c68f6-86545_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"eb274c4ff7160ab891456a1848862fe8c8286a0fc68a50df9724149140c83a36\""
Apr 22 04:28:11 minikube cri-dockerd[1273]: time="2025-04-22T04:28:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-2gfg2_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8c1f111235a02552f61bdd5a973ea64e03badc4420dd4aee893d7100a0ba580a\""
Apr 22 04:28:11 minikube cri-dockerd[1273]: time="2025-04-22T04:28:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-5ktnp_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2afa4d7fefd124be6e3cbc82994c88d13e91168d4986a57307f827a8d5b3fba0\""
Apr 22 04:28:11 minikube cri-dockerd[1273]: time="2025-04-22T04:28:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-dkdsw_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8f6cca311b04d1a7acec01c037b04ce31fd018ebf1260ecdcf180ba9f961e35e\""
Apr 22 04:28:13 minikube cri-dockerd[1273]: time="2025-04-22T04:28:13Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Apr 22 04:28:13 minikube cri-dockerd[1273]: time="2025-04-22T04:28:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2d60a54da65199a95c65be3653f827c1f3f6b14e4df149034caa8bdd65b8e1ab/resolv.conf as [nameserver 192.168.49.1 search NSEROOT.COM options edns0 trust-ad ndots:0]"
Apr 22 04:28:13 minikube cri-dockerd[1273]: time="2025-04-22T04:28:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f54f94ec39020e980ba6005c4acf392b14c23e1e41a2c4d6644412dd872861c0/resolv.conf as [nameserver 192.168.49.1 search NSEROOT.COM options edns0 trust-ad ndots:0]"
Apr 22 04:28:13 minikube cri-dockerd[1273]: time="2025-04-22T04:28:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7d6bb9de3351b5aa05fffc2a9d699b66afa3d9010d881a15d7b07136a3230324/resolv.conf as [nameserver 192.168.49.1 search NSEROOT.COM options ndots:0 edns0 trust-ad]"
Apr 22 04:28:13 minikube cri-dockerd[1273]: time="2025-04-22T04:28:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/32edbcad169e9d90568686c0c23c358808af4fc96a30d0662fb621adf1f30e55/resolv.conf as [nameserver 192.168.49.1 search NSEROOT.COM options edns0 trust-ad ndots:0]"
Apr 22 04:28:13 minikube cri-dockerd[1273]: time="2025-04-22T04:28:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eb2c3854aed42e9c85037dc23b04e384d31902f9437e374276c7cabb68ca3c49/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local NSEROOT.COM options ndots:5]"
Apr 22 04:28:13 minikube cri-dockerd[1273]: time="2025-04-22T04:28:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9ac2475885c1b4bb4be4bc46a5c8613b75e5d8c60934100069e782f2aa56a65e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local NSEROOT.COM options ndots:5]"
Apr 22 04:28:13 minikube cri-dockerd[1273]: time="2025-04-22T04:28:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d7d25b2f818c58e8ee261e7821b45bc0266bce75bef8073da5adebce8c360b27/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local NSEROOT.COM options ndots:5]"
Apr 22 04:28:13 minikube cri-dockerd[1273]: time="2025-04-22T04:28:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eedaf3674a6518c392a30fca2d51aff37a2101912f3b2b3a59abcc51c16793e6/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local NSEROOT.COM options ndots:5]"
Apr 22 04:28:16 minikube cri-dockerd[1273]: time="2025-04-22T04:28:16Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Apr 22 04:28:18 minikube cri-dockerd[1273]: time="2025-04-22T04:28:18Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Apr 22 04:28:43 minikube dockerd[983]: time="2025-04-22T04:28:43.884917912Z" level=info msg="ignoring event" container=41c2a05a6c307fd146b15f4f7efb90447d7ab6c276c5f36590f66f50eb20f723 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 22 04:28:44 minikube dockerd[983]: time="2025-04-22T04:28:44.166078750Z" level=info msg="ignoring event" container=ab828b4e05ee301ff0635f6db8175ff89665989138185093c79d1235bfc2a460 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                   CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
82a64171ba0d4       6e38f40d628db                                                                           13 minutes ago      Running             storage-provisioner         9                   2d60a54da6519       storage-provisioner
85137d258d05a       07655ddf2eebe                                                                           13 minutes ago      Running             kubernetes-dashboard        8                   d7d25b2f818c5       kubernetes-dashboard-7779f9b69b-8bpks
ef84178b67acf       mongo@sha256:2c5f154f4e47ef5008c53ff0c9006b6f5c7d6b119c96015aba4136c50ef48b98           14 minutes ago      Running             mongodb                     2                   9ac2475885c1b       mongodb-deployment-6d9d7c68f6-86545
b0df0f8a3c29a       mongo-express@sha256:1b23d7976f0210dbec74045c209e52fbb26d29b2e873d6c6fa3d3f0ae32c2a64   14 minutes ago      Running             mongo-express               1                   eb2c3854aed42       mongo-express-5dd87b9fcf-qbtm7
32cdf78ca3246       c69fa2e9cbf5f                                                                           14 minutes ago      Running             coredns                     4                   32edbcad169e9       coredns-668d6bf9bc-5ktnp
28dc1aa628c75       115053965e86b                                                                           14 minutes ago      Running             dashboard-metrics-scraper   4                   eedaf3674a651       dashboard-metrics-scraper-5d59dccf9b-dkdsw
ab828b4e05ee3       07655ddf2eebe                                                                           14 minutes ago      Exited              kubernetes-dashboard        7                   d7d25b2f818c5       kubernetes-dashboard-7779f9b69b-8bpks
2b29a2b1d09ea       c69fa2e9cbf5f                                                                           14 minutes ago      Running             coredns                     4                   7d6bb9de3351b       coredns-668d6bf9bc-2gfg2
e19be5e9ee314       040f9f8aac8cd                                                                           14 minutes ago      Running             kube-proxy                  4                   f54f94ec39020       kube-proxy-pxh7n
41c2a05a6c307       6e38f40d628db                                                                           14 minutes ago      Exited              storage-provisioner         8                   2d60a54da6519       storage-provisioner
e3c06a7b707ab       a9e7e6b294baf                                                                           14 minutes ago      Running             etcd                        4                   4e335292c6d0c       etcd-minikube
d05d9bdd31e59       8cab3d2a8bd0f                                                                           14 minutes ago      Running             kube-controller-manager     4                   653764636e369       kube-controller-manager-minikube
9a6fc25f6eef7       a389e107f4ff1                                                                           14 minutes ago      Running             kube-scheduler              4                   1acbb0ad8c175       kube-scheduler-minikube
38f60b5caf86a       c2e17b8d0f4a3                                                                           14 minutes ago      Running             kube-apiserver              4                   d1dd2e025fb73       kube-apiserver-minikube
1d09dbfbc1b26       mongo-express@sha256:1b23d7976f0210dbec74045c209e52fbb26d29b2e873d6c6fa3d3f0ae32c2a64   19 hours ago        Exited              mongo-express               0                   26779fc201e61       mongo-express-5dd87b9fcf-qbtm7
96a656d8f859e       mongo@sha256:2c5f154f4e47ef5008c53ff0c9006b6f5c7d6b119c96015aba4136c50ef48b98           20 hours ago        Exited              mongodb                     1                   eb274c4ff7160       mongodb-deployment-6d9d7c68f6-86545
88eeefc63276f       115053965e86b                                                                           20 hours ago        Exited              dashboard-metrics-scraper   3                   8f6cca311b04d       dashboard-metrics-scraper-5d59dccf9b-dkdsw
11c2ad81de3a4       c69fa2e9cbf5f                                                                           20 hours ago        Exited              coredns                     3                   2afa4d7fefd12       coredns-668d6bf9bc-5ktnp
5192fd621860f       c69fa2e9cbf5f                                                                           20 hours ago        Exited              coredns                     3                   8c1f111235a02       coredns-668d6bf9bc-2gfg2
4c0119adfae12       040f9f8aac8cd                                                                           20 hours ago        Exited              kube-proxy                  3                   9b29660fa1b0d       kube-proxy-pxh7n
cc894544a7b9e       a9e7e6b294baf                                                                           20 hours ago        Exited              etcd                        3                   cdcb0f637abb2       etcd-minikube
9c3fb70568058       a389e107f4ff1                                                                           20 hours ago        Exited              kube-scheduler              3                   136c4b76c3022       kube-scheduler-minikube
b91758854241b       8cab3d2a8bd0f                                                                           20 hours ago        Exited              kube-controller-manager     3                   05685470e9e66       kube-controller-manager-minikube
045eb85414078       c2e17b8d0f4a3                                                                           20 hours ago        Exited              kube-apiserver              3                   69b4a28c9e38b       kube-apiserver-minikube


==> coredns [11c2ad81de3a] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1131273177]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (21-Apr-2025 08:42:40.059) (total time: 30000ms):
Trace[1131273177]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (08:43:10.060)
Trace[1131273177]: [30.000950968s] [30.000950968s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1992553774]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (21-Apr-2025 08:42:40.059) (total time: 30001ms):
Trace[1992553774]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (08:43:10.060)
Trace[1992553774]: [30.001330327s] [30.001330327s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[591606431]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (21-Apr-2025 08:42:40.059) (total time: 30001ms):
Trace[591606431]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (08:43:10.060)
Trace[591606431]: [30.001590385s] [30.001590385s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [2b29a2b1d09e] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[570266641]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (22-Apr-2025 04:28:14.146) (total time: 30001ms):
Trace[570266641]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (04:28:44.148)
Trace[570266641]: [30.001464839s] [30.001464839s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[129071826]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (22-Apr-2025 04:28:14.148) (total time: 30001ms):
Trace[129071826]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (04:28:44.149)
Trace[129071826]: [30.001015188s] [30.001015188s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1930514287]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (22-Apr-2025 04:28:14.146) (total time: 30002ms):
Trace[1930514287]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (04:28:44.149)
Trace[1930514287]: [30.002615859s] [30.002615859s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> coredns [32cdf78ca324] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[30413533]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (22-Apr-2025 04:28:14.147) (total time: 30001ms):
Trace[30413533]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (04:28:44.148)
Trace[30413533]: [30.001044863s] [30.001044863s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1027300110]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (22-Apr-2025 04:28:14.147) (total time: 30002ms):
Trace[1027300110]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (04:28:44.148)
Trace[1027300110]: [30.002064263s] [30.002064263s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[25498614]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (22-Apr-2025 04:28:14.147) (total time: 30002ms):
Trace[25498614]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (04:28:44.149)
Trace[25498614]: [30.002122173s] [30.002122173s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> coredns [5192fd621860] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1197855511]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (21-Apr-2025 08:42:40.059) (total time: 30000ms):
Trace[1197855511]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (08:43:10.060)
Trace[1197855511]: [30.000886879s] [30.000886879s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1865961420]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (21-Apr-2025 08:42:40.059) (total time: 30001ms):
Trace[1865961420]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (08:43:10.060)
Trace[1865961420]: [30.001044545s] [30.001044545s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1314908109]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (21-Apr-2025 08:42:40.059) (total time: 30001ms):
Trace[1314908109]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (08:43:10.060)
Trace[1314908109]: [30.001170879s] [30.001170879s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_04_16T03_14_01_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 15 Apr 2025 21:43:58 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 22 Apr 2025 04:42:20 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 22 Apr 2025 04:40:06 +0000   Tue, 15 Apr 2025 21:43:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 22 Apr 2025 04:40:06 +0000   Tue, 15 Apr 2025 21:43:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 22 Apr 2025 04:40:06 +0000   Tue, 15 Apr 2025 21:43:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 22 Apr 2025 04:40:06 +0000   Tue, 15 Apr 2025 21:43:59 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  490617784Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16154516Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  490617784Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16154516Ki
  pods:               110
System Info:
  Machine ID:                 27ac7eb11a4a4fceac03db17882b5a6d
  System UUID:                56827cf4-3547-4602-b0bb-1f706af1061d
  Boot ID:                    51ba4965-4218-4f25-b8f0-852c00c2547f
  Kernel Version:             6.8.0-57-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     mongo-express-5dd87b9fcf-qbtm7                0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  default                     mongodb-deployment-6d9d7c68f6-86545           0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d15h
  kube-system                 coredns-668d6bf9bc-2gfg2                      100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     6d6h
  kube-system                 coredns-668d6bf9bc-5ktnp                      100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     6d6h
  kube-system                 etcd-minikube                                 100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         6d6h
  kube-system                 kube-apiserver-minikube                       250m (3%)     0 (0%)      0 (0%)           0 (0%)         6d6h
  kube-system                 kube-controller-manager-minikube              200m (2%)     0 (0%)      0 (0%)           0 (0%)         6d6h
  kube-system                 kube-proxy-pxh7n                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d6h
  kube-system                 kube-scheduler-minikube                       100m (1%)     0 (0%)      0 (0%)           0 (0%)         6d6h
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d6h
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-dkdsw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d6h
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-8bpks         0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d6h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%)  0 (0%)
  memory             240Mi (1%)  340Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 14m                kube-proxy       
  Normal   Starting                 14m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  14m (x8 over 14m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    14m (x8 over 14m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     14m (x7 over 14m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  14m                kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 14m                kubelet          Node minikube has been rebooted, boot id: 51ba4965-4218-4f25-b8f0-852c00c2547f
  Normal   RegisteredNode           14m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Apr22 05:03] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000011] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.190501] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000011] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.930916] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000007] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.215367] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000015] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +1.196971] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000011] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.250035] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000011] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[ +10.472764] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000014] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.675343] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000013] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +1.216357] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000006] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.597073] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000011] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.338804] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000012] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.211462] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000011] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +1.435495] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000012] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.463338] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000016] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[Apr22 05:04] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000015] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.295655] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000010] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.482514] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000015] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.418135] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000015] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +1.294290] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000012] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.178685] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000012] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +1.200451] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000012] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.380886] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000007] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.637067] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000006] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.189279] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000010] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +2.013712] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000012] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.162231] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000013] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[Apr22 05:05] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000013] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.238215] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000019] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +4.927740] atkbd serio0: Unknown key pressed (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000014] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.
[  +0.339590] atkbd serio0: Unknown key released (translated set 2, code 0xf6 on isa0060/serio0).
[  +0.000013] atkbd serio0: Use 'setkeycodes e076 <keycode>' to make it known.


==> etcd [cc894544a7b9] <==
{"level":"info","ts":"2025-04-21T08:42:38.463224Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-04-21T08:42:38.464692Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-21T08:42:38.464708Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-21T08:42:38.465048Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-04-21T08:42:38.465241Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-04-21T08:52:38.476776Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16597}
{"level":"info","ts":"2025-04-21T08:52:38.489832Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":16597,"took":"12.857122ms","hash":2566254455,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1748992,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-21T08:52:38.489868Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2566254455,"revision":16597,"compact-revision":15783}
{"level":"info","ts":"2025-04-21T08:57:38.483166Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16835}
{"level":"info","ts":"2025-04-21T08:57:38.484490Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":16835,"took":"1.186795ms","hash":3928857603,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1683456,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-21T08:57:38.484506Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3928857603,"revision":16835,"compact-revision":16597}
{"level":"info","ts":"2025-04-21T09:02:38.487554Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17076}
{"level":"info","ts":"2025-04-21T09:02:38.489057Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":17076,"took":"1.33548ms","hash":1515658002,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1671168,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-21T09:02:38.489075Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1515658002,"revision":17076,"compact-revision":16835}
{"level":"info","ts":"2025-04-21T09:07:38.492778Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17316}
{"level":"info","ts":"2025-04-21T09:07:38.494093Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":17316,"took":"1.178798ms","hash":3587482734,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1646592,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-04-21T09:07:38.494109Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3587482734,"revision":17316,"compact-revision":17076}
{"level":"info","ts":"2025-04-21T09:12:38.497695Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17556}
{"level":"info","ts":"2025-04-21T09:12:38.499060Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":17556,"took":"1.235684ms","hash":3887080061,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1658880,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-21T09:12:38.499077Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3887080061,"revision":17556,"compact-revision":17316}
{"level":"info","ts":"2025-04-21T09:17:38.503008Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17797}
{"level":"info","ts":"2025-04-21T09:17:38.504327Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":17797,"took":"1.194914ms","hash":4027410138,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1634304,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-04-21T09:17:38.504341Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4027410138,"revision":17797,"compact-revision":17556}
{"level":"info","ts":"2025-04-21T09:22:38.508324Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18037}
{"level":"info","ts":"2025-04-21T09:22:38.509769Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":18037,"took":"1.279801ms","hash":390588055,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1654784,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-21T09:22:38.509787Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":390588055,"revision":18037,"compact-revision":17797}
{"level":"info","ts":"2025-04-21T09:27:38.515124Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18278}
{"level":"info","ts":"2025-04-21T09:27:38.516485Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":18278,"took":"1.227238ms","hash":1435072363,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1658880,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-21T09:27:38.516500Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1435072363,"revision":18278,"compact-revision":18037}
{"level":"info","ts":"2025-04-21T09:32:38.519589Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18517}
{"level":"info","ts":"2025-04-21T09:32:38.521009Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":18517,"took":"1.264122ms","hash":3185699207,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1806336,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-04-21T09:32:38.521025Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3185699207,"revision":18517,"compact-revision":18278}
{"level":"info","ts":"2025-04-21T09:37:38.524121Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18785}
{"level":"info","ts":"2025-04-21T09:37:38.526276Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":18785,"took":"1.971764ms","hash":1172632371,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1978368,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-04-21T09:37:38.526307Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1172632371,"revision":18785,"compact-revision":18517}
{"level":"info","ts":"2025-04-21T09:42:38.529237Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19027}
{"level":"info","ts":"2025-04-21T09:42:38.530645Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":19027,"took":"1.253232ms","hash":1746985103,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1683456,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-21T09:42:38.530667Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1746985103,"revision":19027,"compact-revision":18785}
{"level":"info","ts":"2025-04-21T09:47:38.534136Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19267}
{"level":"info","ts":"2025-04-21T09:47:38.535491Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":19267,"took":"1.225679ms","hash":2688870405,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1679360,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-21T09:47:38.535509Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2688870405,"revision":19267,"compact-revision":19027}
{"level":"info","ts":"2025-04-21T09:52:38.540930Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19512}
{"level":"info","ts":"2025-04-21T09:52:38.542861Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":19512,"took":"1.722104ms","hash":1071097688,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1765376,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-04-21T09:52:38.542881Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1071097688,"revision":19512,"compact-revision":19267}
{"level":"info","ts":"2025-04-21T09:57:38.546124Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19752}
{"level":"info","ts":"2025-04-21T09:57:38.548051Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":19752,"took":"1.750612ms","hash":150922774,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1572864,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-04-21T09:57:38.548081Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":150922774,"revision":19752,"compact-revision":19512}
{"level":"info","ts":"2025-04-21T10:08:43.839736Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-04-21T10:08:43.839787Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-04-21T10:08:43.839863Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-21T10:08:43.839934Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-21T10:08:43.839975Z","caller":"v3rpc/watch.go:460","msg":"failed to send watch response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
{"level":"warn","ts":"2025-04-21T10:08:43.840381Z","caller":"v3rpc/watch.go:460","msg":"failed to send watch response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
{"level":"warn","ts":"2025-04-21T10:08:43.840470Z","caller":"v3rpc/watch.go:460","msg":"failed to send watch response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
{"level":"warn","ts":"2025-04-21T10:08:43.982881Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-21T10:08:43.982957Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-04-21T10:08:43.983023Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-04-21T10:08:43.988289Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-21T10:08:43.988437Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-21T10:08:43.988446Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [e3c06a7b707a] <==
{"level":"warn","ts":"2025-04-22T04:28:11.183369Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-22T04:28:11.184086Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-04-22T04:28:11.184166Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-04-22T04:28:11.184182Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-22T04:28:11.184195Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-22T04:28:11.184226Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-22T04:28:11.185904Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-04-22T04:28:11.186363Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-04-22T04:28:11.200830Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"14.195477ms"}
{"level":"info","ts":"2025-04-22T04:28:11.298536Z","caller":"etcdserver/server.go:511","msg":"recovered v2 store from snapshot","snapshot-index":20002,"snapshot-size":"7.9 kB"}
{"level":"info","ts":"2025-04-22T04:28:11.298638Z","caller":"etcdserver/server.go:524","msg":"recovered v3 backend from snapshot","backend-size-bytes":3289088,"backend-size":"3.3 MB","backend-size-in-use-bytes":1236992,"backend-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-04-22T04:28:11.402658Z","caller":"etcdserver/raft.go:540","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":24920}
{"level":"info","ts":"2025-04-22T04:28:11.403017Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-04-22T04:28:11.403060Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 5"}
{"level":"info","ts":"2025-04-22T04:28:11.403085Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 5, commit: 24920, applied: 20002, lastindex: 24920, lastterm: 5]"}
{"level":"info","ts":"2025-04-22T04:28:11.403196Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-22T04:28:11.403384Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-22T04:28:11.403397Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2025-04-22T04:28:11.404440Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-04-22T04:28:11.405006Z","caller":"mvcc/kvstore.go:346","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":19752}
{"level":"info","ts":"2025-04-22T04:28:11.407191Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":20070}
{"level":"info","ts":"2025-04-22T04:28:11.408075Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-04-22T04:28:11.409342Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-04-22T04:28:11.409672Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-04-22T04:28:11.409730Z","caller":"etcdserver/server.go:864","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-22T04:28:11.409799Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-22T04:28:11.409857Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-22T04:28:11.409867Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-22T04:28:11.409951Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-04-22T04:28:11.410434Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-22T04:28:11.412368Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-22T04:28:11.412560Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-22T04:28:11.412573Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-22T04:28:11.413093Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-04-22T04:28:11.413123Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-04-22T04:28:12.203853Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 5"}
{"level":"info","ts":"2025-04-22T04:28:12.203931Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 5"}
{"level":"info","ts":"2025-04-22T04:28:12.204007Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2025-04-22T04:28:12.204035Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 6"}
{"level":"info","ts":"2025-04-22T04:28:12.204060Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2025-04-22T04:28:12.204084Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2025-04-22T04:28:12.204105Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2025-04-22T04:28:12.207757Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-04-22T04:28:12.207796Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-22T04:28:12.207847Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-22T04:28:12.208095Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-04-22T04:28:12.208124Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-04-22T04:28:12.209904Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-22T04:28:12.209899Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-22T04:28:12.211180Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-04-22T04:28:12.211182Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-04-22T04:38:12.231768Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20455}
{"level":"info","ts":"2025-04-22T04:38:12.243379Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":20455,"took":"11.274329ms","hash":3234008121,"current-db-size-bytes":3289088,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1208320,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-04-22T04:38:12.243409Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3234008121,"revision":20455,"compact-revision":19752}


==> kernel <==
 04:42:27 up  2:15,  0 users,  load average: 1.47, 1.34, 1.48
Linux minikube 6.8.0-57-generic #59-Ubuntu SMP PREEMPT_DYNAMIC Sat Mar 15 17:40:59 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [045eb8541407] <==
W0421 10:08:49.709435       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:49.740222       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:49.785753       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:49.794172       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:49.802862       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:49.836359       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:49.863142       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:51.912146       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.131768       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.191861       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.214868       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.292725       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.418109       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.426898       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.501208       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.571055       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.711683       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.713976       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.750842       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.784792       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.851433       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.870306       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.892389       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.918525       1 logging.go:55] [core] [Channel #547 SubChannel #548]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:52.989956       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.003318       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.123077       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.132175       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.151943       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.268655       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.275436       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.280012       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.282585       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.302202       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.308256       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.363613       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.367349       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.396629       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.413377       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.414773       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.455345       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.466158       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.497822       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.497818       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.510469       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.522207       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.531863       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.535416       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.570725       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.594544       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.621258       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.625839       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.640835       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.645337       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.656070       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.667023       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.674089       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.743733       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.775837       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0421 10:08:53.795366       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [38f60b5caf86] <==
I0422 04:28:12.587128       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0422 04:28:12.587153       1 genericapiserver.go:767] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0422 04:28:12.890839       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0422 04:28:12.890864       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0422 04:28:12.890870       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0422 04:28:12.890913       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0422 04:28:12.890857       1 secure_serving.go:213] Serving securely on [::]:8443
I0422 04:28:12.891314       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0422 04:28:12.891324       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0422 04:28:12.891348       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0422 04:28:12.891704       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0422 04:28:12.891717       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0422 04:28:12.897496       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0422 04:28:12.897524       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0422 04:28:12.898068       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0422 04:28:12.898084       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0422 04:28:12.898130       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0422 04:28:12.898136       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0422 04:28:12.898289       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0422 04:28:12.898358       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0422 04:28:12.898389       1 controller.go:119] Starting legacy_token_tracking_controller
I0422 04:28:12.898395       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0422 04:28:12.898407       1 local_available_controller.go:156] Starting LocalAvailability controller
I0422 04:28:12.898411       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0422 04:28:12.898418       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0422 04:28:12.898580       1 aggregator.go:169] waiting for initial CRD sync...
I0422 04:28:12.898599       1 controller.go:78] Starting OpenAPI AggregationController
I0422 04:28:12.898615       1 controller.go:142] Starting OpenAPI controller
I0422 04:28:12.898648       1 controller.go:90] Starting OpenAPI V3 controller
I0422 04:28:12.898670       1 naming_controller.go:294] Starting NamingConditionController
I0422 04:28:12.898710       1 establishing_controller.go:81] Starting EstablishingController
I0422 04:28:12.898733       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0422 04:28:12.898746       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0422 04:28:12.898758       1 crd_finalizer.go:269] Starting CRDFinalizer
I0422 04:28:12.912885       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0422 04:28:12.913065       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0422 04:28:12.973713       1 shared_informer.go:320] Caches are synced for node_authorizer
I0422 04:28:12.981866       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0422 04:28:12.981889       1 policy_source.go:240] refreshing policies
I0422 04:28:12.992019       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0422 04:28:12.998476       1 cache.go:39] Caches are synced for LocalAvailability controller
I0422 04:28:12.998502       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0422 04:28:12.998515       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0422 04:28:12.998488       1 shared_informer.go:320] Caches are synced for configmaps
I0422 04:28:12.998504       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0422 04:28:12.998752       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0422 04:28:13.004143       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0422 04:28:13.011156       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0422 04:28:13.013253       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0422 04:28:13.013291       1 aggregator.go:171] initial CRD sync complete...
I0422 04:28:13.013303       1 autoregister_controller.go:144] Starting autoregister controller
I0422 04:28:13.013309       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0422 04:28:13.013316       1 cache.go:39] Caches are synced for autoregister controller
E0422 04:28:13.034985       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: eced87a3-b377-48bf-b620-500a3c9c3a5d, UID in object meta: "
I0422 04:28:13.214750       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0422 04:28:13.903528       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0422 04:28:16.296535       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0422 04:28:16.447475       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0422 04:28:16.498489       1 controller.go:615] quota admission added evaluator for: endpoints
I0422 04:28:16.601467       1 controller.go:615] quota admission added evaluator for: deployments.apps


==> kube-controller-manager [b91758854241] <==
I0421 08:42:42.234203       1 shared_informer.go:320] Caches are synced for disruption
I0421 08:42:42.234345       1 shared_informer.go:320] Caches are synced for stateful set
I0421 08:42:42.234396       1 shared_informer.go:320] Caches are synced for taint
I0421 08:42:42.234520       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0421 08:42:42.234660       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0421 08:42:42.234765       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0421 08:42:42.235630       1 shared_informer.go:320] Caches are synced for garbage collector
I0421 08:42:42.235636       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0421 08:42:42.235651       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0421 08:42:42.235661       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0421 08:42:42.235801       1 shared_informer.go:320] Caches are synced for PV protection
I0421 08:42:42.235950       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0421 08:42:42.236254       1 shared_informer.go:320] Caches are synced for namespace
I0421 08:42:42.237626       1 shared_informer.go:320] Caches are synced for resource quota
I0421 08:42:42.238164       1 shared_informer.go:320] Caches are synced for daemon sets
I0421 08:42:42.242853       1 shared_informer.go:320] Caches are synced for resource quota
I0421 08:42:42.246026       1 shared_informer.go:320] Caches are synced for garbage collector
I0421 08:42:42.246055       1 shared_informer.go:320] Caches are synced for node
I0421 08:42:42.246141       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0421 08:42:42.246179       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0421 08:42:42.246183       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0421 08:42:42.246187       1 shared_informer.go:320] Caches are synced for cidrallocator
I0421 08:42:42.246245       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 08:42:42.250266       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0421 08:42:42.441037       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="254.250303ms"
I0421 08:42:42.441133       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-6d9d7c68f6" duration="254.449627ms"
I0421 08:42:42.441234       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-6d9d7c68f6" duration="22.071µs"
I0421 08:42:42.441228       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="72.134µs"
I0421 08:42:45.341016       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-6d9d7c68f6" duration="4.87307ms"
I0421 08:42:45.341096       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-6d9d7c68f6" duration="39.591µs"
I0421 08:43:10.526154       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="4.032922ms"
I0421 08:43:10.526228       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="44.392µs"
I0421 08:43:15.562870       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="4.77887ms"
I0421 08:43:15.563077       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="47.133µs"
I0421 08:43:19.882845       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="51.153µs"
I0421 08:43:21.308323       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="6.344996ms"
I0421 08:43:21.308462       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="41.154µs"
I0421 08:43:31.701855       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="6.118799ms"
I0421 08:43:31.702091       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="130.042µs"
I0421 08:47:36.207916       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 08:52:42.530814       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 08:57:49.085012       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:02:55.617485       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:08:02.206299       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:13:09.034887       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:18:15.180483       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:23:22.066823       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:28:28.218332       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:32:08.893492       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="13.850222ms"
I0421 09:32:08.900354       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="6.801834ms"
I0421 09:32:08.900442       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="50.242µs"
I0421 09:32:08.903875       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="35.163µs"
I0421 09:32:30.459697       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="3.907067ms"
I0421 09:32:30.459747       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="27.656µs"
I0421 09:32:42.948054       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:35:26.035323       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:40:32.391126       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:45:38.029259       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:50:44.619964       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0421 09:55:50.709479       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [d05d9bdd31e5] <==
I0422 04:28:16.148847       1 shared_informer.go:320] Caches are synced for daemon sets
I0422 04:28:16.151118       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0422 04:28:16.153115       1 shared_informer.go:320] Caches are synced for persistent volume
I0422 04:28:16.166409       1 shared_informer.go:320] Caches are synced for resource quota
I0422 04:28:16.167463       1 shared_informer.go:320] Caches are synced for ephemeral
I0422 04:28:16.181624       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0422 04:28:16.183755       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0422 04:28:16.184821       1 shared_informer.go:320] Caches are synced for PVC protection
I0422 04:28:16.185958       1 shared_informer.go:320] Caches are synced for service account
I0422 04:28:16.188168       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0422 04:28:16.188244       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0422 04:28:16.188270       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0422 04:28:16.188250       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0422 04:28:16.190493       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0422 04:28:16.192722       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0422 04:28:16.192818       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0422 04:28:16.194150       1 shared_informer.go:320] Caches are synced for cronjob
I0422 04:28:16.194231       1 shared_informer.go:320] Caches are synced for stateful set
I0422 04:28:16.194295       1 shared_informer.go:320] Caches are synced for TTL after finished
I0422 04:28:16.194314       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0422 04:28:16.194427       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="81.718µs"
I0422 04:28:16.196568       1 shared_informer.go:320] Caches are synced for deployment
I0422 04:28:16.196599       1 shared_informer.go:320] Caches are synced for resource quota
I0422 04:28:16.196639       1 shared_informer.go:320] Caches are synced for garbage collector
I0422 04:28:16.196647       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0422 04:28:16.196654       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0422 04:28:16.196747       1 shared_informer.go:320] Caches are synced for HPA
I0422 04:28:16.196561       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="51.429µs"
I0422 04:28:16.198600       1 shared_informer.go:320] Caches are synced for attach detach
I0422 04:28:16.199181       1 shared_informer.go:320] Caches are synced for node
I0422 04:28:16.199241       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0422 04:28:16.199316       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0422 04:28:16.199325       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0422 04:28:16.199330       1 shared_informer.go:320] Caches are synced for cidrallocator
I0422 04:28:16.199391       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0422 04:28:16.200129       1 shared_informer.go:320] Caches are synced for expand
I0422 04:28:16.201295       1 shared_informer.go:320] Caches are synced for endpoint
I0422 04:28:16.202586       1 shared_informer.go:320] Caches are synced for GC
I0422 04:28:16.217817       1 shared_informer.go:320] Caches are synced for garbage collector
I0422 04:28:16.451690       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="257.235822ms"
I0422 04:28:16.451752       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="31.82µs"
I0422 04:28:16.452649       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-6d9d7c68f6" duration="256.647647ms"
I0422 04:28:16.452657       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="256.49423ms"
I0422 04:28:16.452718       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-6d9d7c68f6" duration="36.935µs"
I0422 04:28:16.452750       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="44.718µs"
I0422 04:28:16.625036       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="4.820005ms"
I0422 04:28:16.625114       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="48.641µs"
I0422 04:28:18.661781       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-6d9d7c68f6" duration="4.318908ms"
I0422 04:28:18.661857       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-6d9d7c68f6" duration="39.206µs"
I0422 04:28:45.001529       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="5.513721ms"
I0422 04:28:45.001606       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="41.373µs"
I0422 04:28:46.988139       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="4.666936ms"
I0422 04:28:46.988250       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="69.392µs"
I0422 04:28:53.332311       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="4.429049ms"
I0422 04:28:53.332382       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="38.65µs"
I0422 04:28:54.415495       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="55.282µs"
I0422 04:28:55.137208       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="4.530879ms"
I0422 04:28:55.137291       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="49.497µs"
I0422 04:35:00.364192       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0422 04:40:06.341991       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [4c0119adfae1] <==
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0421 09:54:47.297871       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0421 09:55:17.374204       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0421 09:55:17.374264       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0421 09:55:47.453510       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0421 09:55:47.453554       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0421 09:56:17.524611       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0421 09:56:17.524655       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0421 09:56:47.582087       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0421 09:56:47.582162       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0421 09:57:17.636735       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0421 09:57:17.636779       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0421 09:57:47.721755       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0421 09:57:47.721796       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0421 09:58:17.771582       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0421 09:58:17.771609       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0421 09:58:47.832606       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0421 09:58:47.832651       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"


==> kube-proxy [e19be5e9ee31] <==
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0422 04:38:15.907254       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0422 04:38:45.969288       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0422 04:38:45.969324       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0422 04:39:16.036105       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0422 04:39:16.036155       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0422 04:39:46.107953       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0422 04:39:46.108004       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0422 04:40:16.159206       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0422 04:40:16.159238       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0422 04:40:46.224318       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0422 04:40:46.224349       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0422 04:41:16.275634       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0422 04:41:16.275680       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0422 04:41:46.356154       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0422 04:41:46.356195       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0422 04:42:16.419441       1 proxier.go:1564] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0422 04:42:16.419489       1 proxier.go:833] "Sync failed" ipFamily="IPv6" retryingTime="30s"


==> kube-scheduler [9a6fc25f6eef] <==
I0422 04:28:11.631694       1 serving.go:386] Generated self-signed cert in-memory
I0422 04:28:12.951780       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0422 04:28:12.951808       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0422 04:28:12.957070       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0422 04:28:12.957077       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0422 04:28:12.957164       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0422 04:28:12.957378       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I0422 04:28:12.957475       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0422 04:28:12.957545       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0422 04:28:12.957545       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0422 04:28:12.957680       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0422 04:28:13.058144       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0422 04:28:13.058168       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0422 04:28:13.058153       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController


==> kube-scheduler [9c3fb7056805] <==
I0421 08:42:37.320722       1 serving.go:386] Generated self-signed cert in-memory
W0421 08:42:38.967662       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0421 08:42:38.967693       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0421 08:42:38.967704       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0421 08:42:38.967711       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0421 08:42:38.992739       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0421 08:42:38.992759       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0421 08:42:38.996469       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0421 08:42:38.996544       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0421 08:42:38.996575       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0421 08:42:38.997163       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0421 08:42:39.097369       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0421 10:08:43.863740       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0421 10:08:43.863835       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E0421 10:08:43.863909       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Apr 22 04:28:10 minikube kubelet[1505]: I0422 04:28:10.351304    1505 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Apr 22 04:28:10 minikube kubelet[1505]: I0422 04:28:10.351317    1505 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Apr 22 04:28:10 minikube kubelet[1505]: I0422 04:28:10.351329    1505 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 22 04:28:10 minikube kubelet[1505]: I0422 04:28:10.351348    1505 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 22 04:28:10 minikube kubelet[1505]: I0422 04:28:10.351376    1505 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 22 04:28:10 minikube kubelet[1505]: I0422 04:28:10.351390    1505 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/2b4b75c2a289008e0b381891e9683040-etcd-certs\") pod \"etcd-minikube\" (UID: \"2b4b75c2a289008e0b381891e9683040\") " pod="kube-system/etcd-minikube"
Apr 22 04:28:10 minikube kubelet[1505]: I0422 04:28:10.351404    1505 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 22 04:28:10 minikube kubelet[1505]: I0422 04:28:10.351422    1505 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/d14ce008bee3a1f3bd7cf547688f9dfe-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"d14ce008bee3a1f3bd7cf547688f9dfe\") " pod="kube-system/kube-scheduler-minikube"
Apr 22 04:28:10 minikube kubelet[1505]: I0422 04:28:10.512669    1505 kubelet_node_status.go:76] "Attempting to register node" node="minikube"
Apr 22 04:28:10 minikube kubelet[1505]: E0422 04:28:10.513127    1505 kubelet_node_status.go:108] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Apr 22 04:28:10 minikube kubelet[1505]: E0422 04:28:10.739052    1505 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="800ms"
Apr 22 04:28:10 minikube kubelet[1505]: I0422 04:28:10.914493    1505 kubelet_node_status.go:76] "Attempting to register node" node="minikube"
Apr 22 04:28:10 minikube kubelet[1505]: E0422 04:28:10.914868    1505 kubelet_node_status.go:108] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Apr 22 04:28:11 minikube kubelet[1505]: W0422 04:28:11.022585    1505 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Apr 22 04:28:11 minikube kubelet[1505]: E0422 04:28:11.022683    1505 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
Apr 22 04:28:11 minikube kubelet[1505]: E0422 04:28:11.069686    1505 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events\": dial tcp 192.168.49.2:8443: connect: connection refused" event="&Event{ObjectMeta:{minikube.1838898c08351007  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:minikube,UID:minikube,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:minikube,},FirstTimestamp:2025-04-22 04:28:10.132189191 +0000 UTC m=+0.181320784,LastTimestamp:2025-04-22 04:28:10.132189191 +0000 UTC m=+0.181320784,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:minikube,}"
Apr 22 04:28:11 minikube kubelet[1505]: W0422 04:28:11.279354    1505 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Apr 22 04:28:11 minikube kubelet[1505]: E0422 04:28:11.279453    1505 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
Apr 22 04:28:11 minikube kubelet[1505]: E0422 04:28:11.384160    1505 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Apr 22 04:28:11 minikube kubelet[1505]: E0422 04:28:11.408903    1505 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Apr 22 04:28:11 minikube kubelet[1505]: E0422 04:28:11.416423    1505 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Apr 22 04:28:11 minikube kubelet[1505]: E0422 04:28:11.422969    1505 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Apr 22 04:28:11 minikube kubelet[1505]: I0422 04:28:11.716480    1505 kubelet_node_status.go:76] "Attempting to register node" node="minikube"
Apr 22 04:28:12 minikube kubelet[1505]: E0422 04:28:12.428657    1505 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Apr 22 04:28:12 minikube kubelet[1505]: E0422 04:28:12.428925    1505 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Apr 22 04:28:12 minikube kubelet[1505]: E0422 04:28:12.429022    1505 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Apr 22 04:28:12 minikube kubelet[1505]: E0422 04:28:12.429196    1505 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.015337    1505 kubelet_node_status.go:125] "Node was previously registered" node="minikube"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.015411    1505 kubelet_node_status.go:79] "Successfully registered node" node="minikube"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.015440    1505 kuberuntime_manager.go:1702] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.015894    1505 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.035071    1505 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: E0422 04:28:13.043008    1505 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.043042    1505 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: E0422 04:28:13.048559    1505 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.048583    1505 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: E0422 04:28:13.052932    1505 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.052952    1505 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: E0422 04:28:13.057893    1505 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.130436    1505 apiserver.go:52] "Watching apiserver"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.211269    1505 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/8ef83f2f-dd57-416f-9156-2ecf78429a8d-xtables-lock\") pod \"kube-proxy-pxh7n\" (UID: \"8ef83f2f-dd57-416f-9156-2ecf78429a8d\") " pod="kube-system/kube-proxy-pxh7n"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.211421    1505 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/c2c6b02f-ac4f-46b2-88e7-ce27af88e40c-tmp\") pod \"storage-provisioner\" (UID: \"c2c6b02f-ac4f-46b2-88e7-ce27af88e40c\") " pod="kube-system/storage-provisioner"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.211471    1505 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/8ef83f2f-dd57-416f-9156-2ecf78429a8d-lib-modules\") pod \"kube-proxy-pxh7n\" (UID: \"8ef83f2f-dd57-416f-9156-2ecf78429a8d\") " pod="kube-system/kube-proxy-pxh7n"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.235957    1505 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.439152    1505 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.439228    1505 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: I0422 04:28:13.439474    1505 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: E0422 04:28:13.449359    1505 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: E0422 04:28:13.449357    1505 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Apr 22 04:28:13 minikube kubelet[1505]: E0422 04:28:13.449774    1505 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Apr 22 04:28:16 minikube kubelet[1505]: I0422 04:28:16.969442    1505 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Apr 22 04:28:23 minikube kubelet[1505]: I0422 04:28:23.312853    1505 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Apr 22 04:28:43 minikube kubelet[1505]: I0422 04:28:43.973733    1505 scope.go:117] "RemoveContainer" containerID="d88d32999d71d1c0c23bf5b48f4d0812c0259ac0d1ff35c82623e366a3908b87"
Apr 22 04:28:43 minikube kubelet[1505]: I0422 04:28:43.973968    1505 scope.go:117] "RemoveContainer" containerID="41c2a05a6c307fd146b15f4f7efb90447d7ab6c276c5f36590f66f50eb20f723"
Apr 22 04:28:43 minikube kubelet[1505]: E0422 04:28:43.974100    1505 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(c2c6b02f-ac4f-46b2-88e7-ce27af88e40c)\"" pod="kube-system/storage-provisioner" podUID="c2c6b02f-ac4f-46b2-88e7-ce27af88e40c"
Apr 22 04:28:44 minikube kubelet[1505]: I0422 04:28:44.987391    1505 scope.go:117] "RemoveContainer" containerID="008a71eb8e86d2536a5cb8b5308b52a50ab910e78de982c4e10f83efa6a99dac"
Apr 22 04:28:44 minikube kubelet[1505]: I0422 04:28:44.987623    1505 scope.go:117] "RemoveContainer" containerID="ab828b4e05ee301ff0635f6db8175ff89665989138185093c79d1235bfc2a460"
Apr 22 04:28:44 minikube kubelet[1505]: E0422 04:28:44.987784    1505 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-7779f9b69b-8bpks_kubernetes-dashboard(9704c9f7-5e9c-4e67-814e-75fe662cbce0)\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-8bpks" podUID="9704c9f7-5e9c-4e67-814e-75fe662cbce0"
Apr 22 04:28:54 minikube kubelet[1505]: I0422 04:28:54.401004    1505 scope.go:117] "RemoveContainer" containerID="ab828b4e05ee301ff0635f6db8175ff89665989138185093c79d1235bfc2a460"
Apr 22 04:28:59 minikube kubelet[1505]: I0422 04:28:59.174232    1505 scope.go:117] "RemoveContainer" containerID="41c2a05a6c307fd146b15f4f7efb90447d7ab6c276c5f36590f66f50eb20f723"


==> kubernetes-dashboard [85137d258d05] <==
2025/04/22 04:28:54 Using namespace: kubernetes-dashboard
2025/04/22 04:28:54 Using in-cluster config to connect to apiserver
2025/04/22 04:28:54 Using secret token for csrf signing
2025/04/22 04:28:54 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/04/22 04:28:54 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2025/04/22 04:28:54 Successful initial request to the apiserver, version: v1.32.0
2025/04/22 04:28:54 Generating JWE encryption key
2025/04/22 04:28:54 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2025/04/22 04:28:54 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2025/04/22 04:28:54 Initializing JWE encryption key from synchronized object
2025/04/22 04:28:54 Creating in-cluster Sidecar client
2025/04/22 04:28:54 Successful request to sidecar
2025/04/22 04:28:54 Serving insecurely on HTTP port: 9090
2025/04/22 04:28:54 Starting overwatch


==> kubernetes-dashboard [ab828b4e05ee] <==
2025/04/22 04:28:14 Using namespace: kubernetes-dashboard
2025/04/22 04:28:14 Using in-cluster config to connect to apiserver
2025/04/22 04:28:14 Using secret token for csrf signing
2025/04/22 04:28:14 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/04/22 04:28:14 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc0007dfae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000518080)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf


==> storage-provisioner [41c2a05a6c30] <==
I0422 04:28:13.857414       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0422 04:28:43.867223       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [82a64171ba0d] <==
I0422 04:28:59.263442       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0422 04:28:59.270742       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0422 04:28:59.271169       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0422 04:29:16.670965       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0422 04:29:16.671103       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_289a95e1-359f-4544-9c5c-fbbb484a2913!
I0422 04:29:16.671089       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"d7c82833-7847-4e18-87a6-529b7b6c97b9", APIVersion:"v1", ResourceVersion:"20268", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_289a95e1-359f-4544-9c5c-fbbb484a2913 became leader
I0422 04:29:16.771778       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_289a95e1-359f-4544-9c5c-fbbb484a2913!

